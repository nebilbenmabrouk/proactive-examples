<?xml version="1.0" encoding="UTF-8"?>
<job
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xmlns="urn:proactive:jobdescriptor:3.12" xsi:schemaLocation="urn:proactive:jobdescriptor:3.12 http://www.activeeon.com/public_content/schemas/proactive/jobdescriptor/3.12/schedulerjob.xsd"  name="Python_Spark_Pi" projectName="Basic Big Data" priority="normal" onTaskError="continueJobExecution"  maxNumberOfExecution="2"  >
  <variables>
    <variable name="nb_random_points" value="10000" />
  </variables>
  <description>
    <![CDATA[ A workflow to submit a PySpark job, i.e. a Python Spark job, to estimate Pi. ]]>
  </description>
  <genericInformation>
    <info name="workflow.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/spark.png"/>
  </genericInformation>
  <taskFlow>
    <task name="Python_Task" 
    
    
    
    
    fork="true">
      <description>
        <![CDATA[ The simplest task, ran by a Python engine. ]]>
      </description>
      <genericInformation>
        <info name="task.icon" value="/automation-dashboard/styles/patterns/img/wf-icons/spark.png"/>
      </genericInformation>
      <forkEnvironment javaHome="/usr" >
        <envScript>
          <script>
            <file url="${PA_CATALOG_REST_URL}/buckets/scripts/resources/fork_env_docker_dlm3/raw" language="groovy"></file>
          </script>
        </envScript>
      </forkEnvironment>
      <scriptExecutable>
        <script>
          <code language="cpython">
            <![CDATA[
import os
os.environ["SPARK_USER"] = "root"

import sys
from random import random
from operator import add
from pyspark.sql import SparkSession


def f(_):
        x = random() * 2 - 1
        y = random() * 2 - 1
        return 1 if x ** 2 + y ** 2 <= 1 else 0
spark = SparkSession.builder\
                    .appName("PythonPi")\
                    .config("spark.jars.ivy","/tmp/.ivy")\
                    .getOrCreate()
sparkContext = spark.sparkContext
sparkContext.setLogLevel('ERROR')
             
n = int(variables.get('nb_random_points'))
partitions = 2
count = sparkContext.parallelize(range(1, n + 1), partitions)\
                    .map(f)\
                    .reduce(add)

print("Pi is roughly %f" % (4.0 * count / n))

spark.stop()
]]>
          </code>
        </script>
      </scriptExecutable>
      <metadata>
        <positionTop>
            241
        </positionTop>
        <positionLeft>
            198.5
        </positionLeft>
      </metadata>
    </task>
  </taskFlow>
  <metadata>
    <visualization>
      <![CDATA[ <html>
    <head>
    <link rel="stylesheet" href="/studio/styles/studio-standalone.css">
        <style>
        #workflow-designer {
            left:0 !important;
            top:0 !important;
            width:2830px;
            height:3392px;
            }
        </style>
    </head>
    <body>
    <div id="workflow-visualization-view"><div id="workflow-visualization" style="position:relative;top:-236px;left:-193.5px"><div class="task _jsPlumb_endpoint_anchor_ ui-draggable active-task" id="jsPlumb_1_4" style="top: 241px; left: 198.5px;"><a class="task-name" data-toggle="tooltip" data-placement="right" title="The simplest task, ran by a Python engine."><img src="/automation-dashboard/styles/patterns/img/wf-icons/spark.png" width="20px">&nbsp;<span class="name">Python_Task</span></a></div><div class="_jsPlumb_endpoint source-endpoint dependency-source-endpoint connected _jsPlumb_endpoint_anchor_ ui-draggable ui-droppable" style="position: absolute; height: 20px; width: 20px; left: 239px; top: 271px;"><svg style="position:absolute;left:0px;top:0px" width="20" height="20" pointer-events="all" position="absolute" version="1.1" xmlns="http://www.w3.org/1999/xhtml"><circle cx="10" cy="10" r="10" version="1.1" xmlns="http://www.w3.org/1999/xhtml" fill="#666" stroke="none" style=""></circle></svg></div></div></div>
    </body>
</html>
 ]]>
    </visualization>
  </metadata>
</job>